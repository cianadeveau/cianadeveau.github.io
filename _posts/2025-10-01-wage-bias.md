---
title: 'Wage Bias in LLMs'
date: 2025-10-01
permalink: /posts/2025/10/wage-bias/
tags:
  - mech interp
  - activation patching
  - compositional circuit

---

Detecting and correcting wage bias in LLMs. 

Wage Bias in LLMs
======
Background
------
Much like the brain, large language models learn the statistics of the environment they are exposed to, and this inherently comes with quite a bit of bias. As LLMs are integrated more acutely into peopleâ€™s daily life, it is important to try and minimize bias so that each user receives genuinely helpful responses. While tackling bias at large is a hefty task, a recent paper (Sorokovikova et al. 2025) highlighted a tractable direction to start with: socio-economic bias in wage negotiations. They found that when an LLM is prompted to provide salary advice, it will substantially decrease the recommended negotiation value for women and people of color. Here, this work attempts to begin chipping away at these biases by investigating the potential circuits underlying the gender differences in particular. 

Key Takeaways
------
1. Even small models such as GPT-2 small show bias in salary recommendations.
2. While the contributions to this bias are distributed in the model, there are a select number of attention heads that strongly affect the salary output.
3. One attention head in particular showed an attention pattern that robustly links salary--> geneder-like tokens. Causal interventions on this head reduce the bias in the output.

Results
------
This study uses GPT-2 Small, so it was first necessary to test whether this bias exists in this model. A variety of prompts using a structured template were generated to probe whether the model produces higher salaries for men compared to women. Example prompts (Fig. 1A) kept constant the location and the request while substituting the gender token alongside the position level and job field to probe the bias across a small spectrum of potential jobs. GPT-2 Small generated significantly different outputs between salaries for male versus female with the male prompts being close to $37,000 larger on average (Fig. 1B, t-test, p<0.05). Therefore, it is possible to investigate bias in this model. 
![Figure 1](/images/bp1/Fig1-v1.png)
