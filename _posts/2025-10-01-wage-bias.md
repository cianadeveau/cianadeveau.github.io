---
title: 'Wage Bias in LLMs'
date: 2025-10-01
permalink: /posts/2025/10/wage-bias/
tags:
  - mech interp
  - activation patching
  - compositional circuit

---

Detecting and correcting wage bias in LLMs. 

Wage Bias in LLMs
======
Background
------
Much like the brain, large language models learn the statistics of the environment they are exposed to, and this inherently comes with quite a bit of bias. As LLMs are integrated more acutely into people’s daily life, it is important to try and minimize bias so that each user receives genuinely helpful responses. While tackling bias at large is a hefty task, a recent paper [1] highlighted a tractable direction to start with: socio-economic bias in wage negotiations. They found that when an LLM is prompted to provide salary advice, it will substantially decrease the recommended negotiation value for women and people of color. Here, this work attempts to begin chipping away at these biases by investigating the potential circuits underlying the gender differences in particular. 

Key Takeaways
------
1. Even small models such as GPT-2 small show bias in salary recommendations.
2. While the contributions to this bias are distributed in the model, there are a select number of attention heads that strongly affect the salary output.
3. One attention head in particular showed an attention pattern that robustly links salary--> geneder-like tokens. Causal interventions on this head reduce the bias in the output.

Results
------
<p>This study uses GPT-2 Small, so it was first necessary to test whether this bias exists in this model. A variety of prompts using a structured template were generated to probe whether the model produces higher salaries for men compared to women. Example prompts (Fig. 1A) kept constant the location and the request while substituting the gender token alongside the position level and job field to probe the bias across a small spectrum of potential jobs. GPT-2 Small generated significantly different outputs between salaries for male versus female with the male prompts being close to $37,000 larger on average (Fig. 1B, t-test, p<0.05). Therefore, it is possible to investigate bias in this model. 
</p>
<img src="/images/bp1/Fig1-v1.png" alt="Figure 1" width="75%" height="75%">
<p>
<em><b>Figure 1: Wage Bias in GPT-2 Small.</b> (A) Generated prompts. Key details in the prompt including gender, level and job field were selected from a list of options. As the variance could be large, 5 trials per combination were run resulting in 33 trials per gender after cleaning the outputs (valid salaries were defined as the range from 20,000 to 500,000 and all others were excluded). (B) Cumulative density plot displaying salary distributions for male and female prompts. </em>
</p>
<p>A preliminary calculation of the contributions each layer provides to the ‘ $’ token, as monetary values often are preceded by this token. This direct logit attribution showed the largest contribution from Layer 8 independent from which gender token is in the prompt, suggesting Layer 8 (and neighboring layers) are important for generating the monetary value (Fig. 2A). Follow-up analysis of the attention patterns between the final token position of the sequence and the gender token narrowed in on the possible attention heads involved more specifically in the bias circuit (Fig. 2B,C). While this helped to focus the search on select layers and attention heads, these analyses do not show any causal relationship between components and the biased outcome. 
</p>
<img src="/images/bp1/Fig2-v1.png" alt="Figure 2">
<p>
<em><b>Figure 2: Correlational analysis reveals a select set of layers and attention heads potentially involved in the bias circuit.</b> (A) Direct Logit Attribution analysis. Relative change from previous layer contributions for the ‘ $’ token in both prompts including male and female tokens. (B) Average attention pattern values for the final token in the prompt to the gender token for all attention heads. Sorted by attention value. (C) Top 5 attention heads with the highest attention values.  
</em>
</p>
<p>To causally the top 5 heads’ roles, their softmaxed attention pattern was ablated to eliminate their individual contribution to the overall computation. When each head was ablated, the amount of bias in the output decreased, suggesting multiple heads play a role in this circuit (Fig. 3A). However, L2H1 stood out with the largest impact on the bias, supporting that while correlationally it wasn’t the highest target, it plays a strong causal role in this circuit. 
</p>
<img src="/images/bp1/Fig3-v1.png" alt="Figure 3" width="47%" height="47%">
<p>
<em><b>Figure 3: Ablation of attention heads reveals casual contribution to bias circuit.</b> (A) Ablation of top 5 attention heads individually. Comparing difference in the average salary prediction for male vs female prompts in ablated trials to baseline without any ablated heads. Bias difference calculated as Male-Female. Change calculated as (ablated_bias-baseline_bias)/baseline_bias.</em>
</p>
<p>It’s possible then that this is a compositional circuit where the final token is attending to an intermediate token that attends to the gender and the former computational step could be happening in L2H1 (Fig. 4A). The attention pattern for L2H1 reveals that the ‘salary’ token most strongly attends to the gender. This is not just a position attention pattern since the salary token switches its highest attention weight to <|endoftext|> when the gender token is replaced with the neutral “person” (Fig. 4B). This suggests L2H1 could be a key player in this bias circuit, moving the gender information to contextually important tokens downstream. 
</p>
<img src="/images/bp1/Fig4-v1.png" alt="Figure 4" width="65%">
<p>
<em><b>Figure 4: Testing for compositional pathway: salary to gender token connection in L2H1.</b> (A) Schematic representing the direct vs indirect pathways for the next token prediction. (B) The attention pattern for the salary token in L2H1 for three prompts. Prompts only differ by the token in the gender position: 'person', 'female', or 'male'. Highest weight highlighted by dark blue box.</em>
</p>
<p>To causally connect this head to the bias circuit a series of ablations were carried out targeting different levels involving this attention head to see how specific the ablation could be and still be effective. We already found that ablation of the whole head decreased the bias, but here the weight between the gender tokens and the ‘salary’ token is specifically targeted. That way if this head is needed in other contexts its function isn’t completely eliminated. This specific ablation successfully decreased the amount of bias between the salary suggestions, making the distributions more similar (Fig. 5A,B).
</p>
<img src="/images/bp1/Fig5-v1.png" alt="Figure 5" width="45%" height="45%">
<p>
<em><b>Figure 5: Ablating attention weight between salary and gender token in L2H1.</b> (A) Baseline distributions for each prompt. Dotted line: mean. (B) Distribution of salaries when the L2H1 salary to gender attention weight is ablated. </em>
</p>
<p>Of course, prompts do not always follow a strict template and the specific words used could vary slightly. To test for the robustness of L2H1, the attention patterns for the salary token were tested against a range of words. L2H1 strongly and specifically activates for words semantically similar to “male” and “female”, while not activating for a handful of other nouns and adjectives that could fit the prompt (Fig. 6A,B). Expanding the ablations on this head could then help decrease the bias across a variety of prompts. To make this ablation more robust to possible prompts, the nearest neighbors in the embedding space for the tokens ‘male’ and ‘female’ were calculated so the ablation function can more thoroughly address potential bias even when prompted with different words. Employing the same ablation function across prompts with different sets of gender tokens successfully decreased the amount of bias in each case. 
</p>
<img src="/images/bp1/Fig6-v1.png" alt="Figure 6" width="75%" height="75%">
<p>
<em><b>Figure 6: Ablating attention weight between salary and semantically similar gender tokens in L2H1. </b> (A) The attention pattern for the salary token in L2H1 for multiple prompts differing by the applicant noun. (B) The attention pattern for the salary token in L2H1 for multiple prompts differing by the applicant adjective. (C) Comparison of baseline bias for a variety of gender-related nouns to output when L2H1’s salary to gender token connection is ablated. </em>
</p>
Conclusions, Limitations, and Future Directions
------
Bias in training data can be sneaky and hard to eliminate leaving us with models that will produce biased outputs as a result. While it is incredibly difficult to address the range of possibilities all at once, we can make progress on sub-tasks and hopefully learn principles that could apply at large as we learn how to properly address correcting biases. Here, this work honed in on gender wage bias. Using both correlational and causal techniques, a specific attention head was found that ties gender information to salary predictions. Ablating this connection successfully decreases the bias output, making the distributions for the generated salaries similar regardless of the gender. 

It’s important to note though that this head was not the only one that had an impact on this bias, so like most concepts, this bias is distributed in the model and may require intervention on multiple heads for a more complete correction. This may not be the exact mechanism that underlies other wage biases, but trying to disconnect the irrelevant context to tokens involving salary could prove to be a fruitful direction in the future. And while this work showed some promising results, there is obviously more bias out there than just the gender wage gap. These experiments could and should be expanded to investigate additional socio-economic bias. 

These experiments were conducted in GPT-2 Small which comes with a few technical limitations to note. First, the variance is quite high for the responses from this model. The baseline bias was recalculated for each experiment and fluctuated quite a bit. With that in mind it’s important to note that the change in bias produced from ablations likely also varies from trial to trial so some attention heads may have stronger or weaker roles than shown here. It’s possible these deviations narrow in larger models, so future work expanding these experiments into larger models is needed. With a larger model comes far more parameters in which case we might expect a more distributed and complex circuit. Overall, it was promising to find an attention head with such a robust attention pattern between gender and salary, but future work flushing out more of the compositional circuit will help provide a more complete picture of how these models integrate bias. 

References
------
[1] Sorokovikova, Aleksandra, Pavel Chizhov, Iuliia Eremenko, and Ivan P. Yamshchikov. 2025. “Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models.” arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2506.10491.

